# 操作系统

**一、什么是操作系统？**

答：操作系统是软件程序。

​	BIOS在加电之后自动将操作系统代码从磁盘读入内存后运行，在这个过程中，伴随着一些数据结构的生成，它们是关于硬件设备的信息，操作系统通过它们来**感知和管理计算机硬件**。现代操作系统里说：”操作系统屏蔽掉了丑陋参差的底层细节，只为上层的应用程序提供美丽整齐的接口。”

**二、什么是系统调用？调用的流程**

答：系统调用函数是操作系统和应用程序之间的接口。

​	应用程序大多数时间运行在用户态，遇到了与系统资源相关的操作就要进入内核态，而系统调用是从用户态进入内核态的唯一合法途径，进入内核态才能使用内核指令，进而控制硬件。

应用程序函数--------系统调用函数------->库函数（能展成含有中断的代码，通过中断进入内核）------->调用内核里的指令控制硬件

**三、操作系统如何管理CPU？**

答：多进程的推进与调度。

​	每个进程都有一个存放信息的结构PCB记录它的执行情况、地址空间映射等新信息。操作系统只要把这些PCB记录好(形成队列)，按照合理的次序调度推进就形成了多进程图像，就完成了CPU的管理。CPU忙碌起来了，计算机才能快，外设硬件才能被充分带动。

**四、程序与进程的区别？**

答：程序是编译好后存放在磁盘上的固定指令序列，进程是装入内存中的执行起来的程序。

​	它们是不一样的，程序是死的静态概念，而进程能够走走停停，来回切换，有开始与结束是动态概念，并且它们的所有不一样都存放在PCB(进程控制块)这个数据结构中。

**五、进程与线程的联系与区别？**

答：一个进程中可以包含多个线程，线程像是一种轻量级的进程。

​	进程之间会发生**调度且倾向于独立**，每个进程都有自己单独的程序计数器PC，地址空间映射表，文件句柄等大量数据结构，因此进程的切换要切换上述的所有信息。地址空间分离避免了它们在内存中发生冲突。

​	线程之间也会**调度但更倾向于合作**，因此线程会共享进程范围内大量的资源。每个线程仅有自己的程序计数器PC和运行时栈及栈中的变量，线程的切换只会切换PC和栈。这些线程能访问到很多相同的变量，并在同一个堆上分配对象。

**六、线程的状态有哪些？**

答：新，就，运，阻，终。

​	以java为例。**新建状态**对应着线程对象刚刚被创建，还没有执行。此时调用线程的start()方法，会进入**就绪状态**，代表该线程开始进入就绪队列开始申请CPU时间片。当线程获得CPU时间后，它才进入**运行状态**，真正开始执行run()方法。若该线程并未结束，但由于某些原因挂起让出了CPU时间片，此时进入**阻塞状态**。线程自然结束或者执行时发生了未捕获的异常后，变为**终止状态**。

**注意：**进入阻塞状态和就绪状态的条件往往相反，如：IO的发生与返回，sleep()睡着了与睡醒了，同步锁的索要与获得，等待与收到通知。

**七、进程间通信的方式？**

​	答：进程可以通过 信号、信号量、管道、套接字，共享内存、消息队列的方式通信协作。

**信号**被某个进程产生，并设置此信号传递的对象(pid)，然后传递给操作系统；操作系统根据接收进程的设置（是否阻塞）而选择性的发送给接收者，目的进程接收到此信号后，保存上下文转而执行对应的中断服务程序，执行完成后再返回。

**信号量**是一些共享数据，每个进程生产或消费资源时修改信号量，信号量符合要求时进程才能推进。它用来保护临界区，以完成与其他进程之间的同步。

**管道**的实质是一个缓冲区，进程以严格的先进先出的方式从缓冲区存取数据。匿名管道是在内存中，只能在有亲缘关系的进程间通信；有名管道以磁盘文件的方式存在，可以实现本机任意两个进程通信。

**消息队列**是不同进程消息存放的队列，可以实现消息的随机查询**克服了**信号承载信息量少，管道只能承载无格式字节流以及缓冲区大小受限等缺点。

 **套接字(socket)** 是一种通信机制，客户端/服务端进程的通信既可以在本地单机上进行，也可以跨网络进行。也就是说它可以让同一计算机上的进程通信，也可以让不在同一台计算机但通过网络连接计算机上的进程进行通信。

**八、线程间的同步是什么、同步原则及同步方式？**

​	答：多线程完成一个任务，需要访问共享数据，彼此间这种合作需要合理有序地推进。

原则要保证：互斥进入，有空让进，有限等待。

信号：某些指令的推进需要一些前提，需要其他线程发送给它一个**信号**，确保执行的正确性。线程接收到信号之前处于阻塞状态，接收到信号之后才被唤醒。

信号量：只发送信号不能完成多个线程之间的同步，需要用**信号量**携带更多的信息，使线程合理地运行(阻塞，调度，唤醒)

互斥量(Mutex)：互斥量是一把锁，某一时刻只能由一个线程持有，只有持有它的线程才有访问共享资源的权限。

**九、常见的进程调度算法？**

FCFS:先来先服务

SJF:短作业优先

RR:时间片轮转

一个系统肯定又有前台任务，又有后台任务，它们所需的满足不同，只能使用一种折中的方案

前台任务-IO密集型，实际运算较少，更关注响应时间优先级大

后台任务-CPU密集型，实际运算较多，更关注周转时间优先级小

**多级反馈队列：** FCFS、SJF和优先级调度算法仅对某一类任务有利，相比之下，多级反馈队列调度算法同时兼顾不同类型的任务。维护多个优先级不一样的队列，对于优先级最低的队列来说，里面是遵循**时间片轮转**法。对于其他队列，遵循的是**先来先服务**算法，每一进程分配一定的时间片，若时间片运行完时进程未结束，则进入下一优先级队列的末尾。

​	**该算法设计的精妙在于**：各个队列的时间片是随着优先级的增加而减少的优先级越高的队列中它的时间片就越短。前台任务在高优先级队列中用小时间片快速响应，后台任务在低优先级队列中用较长时间片专注于处理。

**十、什么是死锁？会引起什么后果？产生死锁的必要条件是什么？**

​	答：多个进程在内存中同时推进，交替执行形成了环路等待(某个进程的推进依赖一些条件，而这些条件同也依赖于它自身的推进)，或者说多个进程由于互相等待彼此所持有的资源，且都不释放自己手中的资源造成的谁也无法执行的情况。**java中**自己设计的锁导致锁的嵌套、同步代码块的粒度过大、多个功能共用使同一把锁常常会**影响性能甚至带来死锁**。很多进程无法执行处于阻塞状态，此时CPU没做正经工作因此利用率极低。

**四个必要条件：**资源互斥使用、不可抢占、保持且请求、循环等待。

**十一、处理死锁的策略？**

**死锁预防：**破坏死锁出现的条件。例如一次性申请所有资源。进程不会占有部分资源再去申请其它资源。问题是许多资源分配后很长时间才真正利用上，资源利用率较低。

**死锁避免：**检测每个资源请求，如果可能造成死锁就拒绝。每次请求出现时，假装分配给它资源，然后调用**Banker算法**，若求不出来可执行序列就意味着分配给它会带来死锁，所以千万不能分配给他！

**死锁检测和恢复：**检测到死锁出现时，让一些进程回滚让出资源。定期检查或者CPU利用率低时检测，用**Banker算法**检测出死锁的进程，让它们中的某些进程回滚。恢复比较复杂

**死锁忽略：**全然不顾，进程卡死就卡死，太卡了就重启。PC上死锁忽略的处理代价很小，进程少出现死锁的概率也很低，因此是非常实惠的，但是在服务器上不行。

**十二、操作系统如何管理内存？**

​	答：**内存是为进程服务的**，在程序变为进程的那一刻开始就伴随着代码和数据装入内存，进程运行或者调度都伴随着内存中数据的装入或者换出。为了使进程执行的更流畅，采用虚拟内存、分段分页，快表等机制以及合理的页面替换算法来提高内存空间的利用率和访问内存的速度，这就完成了内存管理的主要内容。

**十三、内存管理机制有哪些？**

​	答：简单分为**连续分配管理方式**和**非连续分配管理方式**这两种。连续分配管理方式是指为每个用户程序分配一个连续的内存空间如**分区管理** 。同样地，非连续分配管理方式允许程序使用的内存分布在离散或者说不相邻的内存中，常见的如**分页管理**和**分段管理**。

分区机制：将内存分为一个个区，如果进程需要内存的话，操作系统就分配给它一个连续分区。这会造成内存的碎片化-总空间够用，但是没有一块完整的连续地址处理新的内存请求，可能需要内存紧缩移动。非常非常耗时

分段机制：程序不是整个装入内存的，而是分成好几个段，每个段都有各自的特点：代码段只读，数据段可写，栈会动态增长，将各个段分别放入内存以便用户分开治理使用。进程的PCB中的段表(**局部描述符表LDT**)对应了该进程的多个段分别存放于内存中的何处。

分页机制：将程序和内存都分成一页一页的，将面包切成片，从连续变成离散最大的浪费不过4K。划分力度更大，提高了内存利用率，减少了碎片。进程PCB中有属于自己的**页表**，管理逻辑页到物理页的映射。

**十四、** **分页机制和分段机制有哪些共同点和区别？**

**共同点**

页和段都是内存管理机制中引入的概念，两者都是离散分配内存的方式。但每个页和段中的内存是连续的，可以通过基址加偏移寻址

**区别**

页的大小是固定的，由操作系统决定；而段的大小不固定，取决于我们当前运行的程序。

分页仅仅是为了操作系统向下对真实物理内存管理的需求。而段是带有逻辑信息单位，在程序中可以体现为代码段，数据段，能够更好向上满足用户的需要。

**十五、段页结合式管理机制？**   

​	机制：将程序的各个段放入虚拟内存中合适的位置，由进程的**LDT**表示上述位置映射关系。虚拟内存的地址空间需要分页，每个页对应于真实物理内存中的页框，由进程的**页表**这个映射关系。

​	流程：运行过程中某条指令中含有地址值(**逻辑地址**)需要重定位，用这条指令的所在的段号查段基址，加上段内偏移组成段地址(**虚拟地址**)，用虚拟地址的页号查页表得到页框号，加上页内偏移得到真实的**物理地址**。运行时重定位完毕，指令得以正常运行，进程得以推进，也正是在进程推进的过程中带动了内存的使用。

注意，因为我们有段表和页表，MMU可以自主完成上述计算。

**十六、什么是虚拟内存(虚拟寻址)，为什么引入虚拟内存？**

​	答：虚拟内存是对物理内存的抽象管理机制。现代操作系统中用户和进程只能感知操作到虚拟内存、看不见更底层的物理内存。

1、OS要向用户程序提供分段机制，又要向实际物理内存提供分页机制，所以引入虚拟内存作为中间的媒介连接两种机制。

2、虚拟内存使得应用程序认为它占有着连续的内存(**一种自己在独享主存的错觉**),甚至已经占有了超过实际物理内存的地址空间，而实际上它占用的物理内存不一定是连续的，甚至还有部分暂存在外部磁盘存储器上，在需要时进行数据交换。

3、如果直接把物理地址暴露出来的话会带来严重问题，比如可能对操作系统造成伤害以及给同时运行多个程序造成困难。引入虚拟内存再配合不同进程各自的LDT和页表就完成了地址空间的分离：一个进程中无法影响另一进程使用的物理内存。

**十七、分页内存的核心问题是什么？引入多级页表和快表的原因？**

​	在分页内存管理中，很重要的两点是：1、解决虚拟地址空间大，页表也会很大的问题。2、虚拟地址到物理地址的转换要快。

​	为了提高内存利用率页应该尽量小，但虚拟地址空间大页表项很多，整个页表会在内存中占用过多空间。但页表项是连续的，那么就可以采取章节索引的思想，通过高级页表避免了很多根本就用不上的页表项放入内存。多级页表虽然提高了空间性能，但是降低了时间，尤其是64位系统，为了得到某一个地址的内容多级页表带来了多次访问内存，这也是比较耗时的。

​	TLB就是为了解决这样的问题，它是一组相联存储寄存器，因此里面的表项可以不是连续的，不必遍历，通过硬件就可直接定位页号进而找到页框号，快速完成虚拟地址到物理地址的翻译。它好比人脑，最近我在看书，与其每次同看多级目录查找，不如干脆就记住要看书的页码。

**十八、程序运行的局部性原理？**

​	答：我们的程序在执行的时候往往呈现局部性规律：在某个较短的时间段内，程序局限于反复执行某一部分的指令，反复使用某一部分

的数据，总之程序访问的存储空间局限于某个区域。把最近使用的数据块放到Cache中，把最近访问的页的映射保存在快表中，充分利用

程序运行的局部性原理后，人们建立的操作系统内存管理机制 与 高速缓存--内存--磁盘的存储体系结构 成本和性能俱佳。

**十九、常见的页面换入换出算法？**

​	答：当进程发起对虚拟内存中的地址进行访问的时候，会查页表通过虚拟内存的页号找到物理内存的页框号。若没有映射关系，就会发出“缺页中断”，从磁盘中找到对应的页装入物理内存。物理内存有限，当存满时不得不发生页面替换，这需要合理的页面替换算法：

FIFO：选最先进入的页面最先淘汰，基于队列实现比较容易，但是性能不好。

OPT：选最远将使用的页淘汰，是最优方法，但是需要直到未来发生的事儿，这不可能。

LRU：选最近这段时间内没有使用过的页淘汰，利用程序的局部性原理预测未来，可与OPT的性能相媲美。

LRU的准确实现：基于时间戳的实现需要寻找最小时间，基于页码栈实现需要进行栈中元素移动，这两种方法实现起来代价都比较大

LRU的近似实现：二次机会算法、Clock算法（（ Not recently used）最近未用过）

二次机会是给每一页关联一个使用位，所有页的使用位组成一个循环的缓冲区。当某一页首次装入主存或被访问到了使用位设置为1。当需要替换一页时，操作系统扫描循环缓冲区，每当遇到一个使用位为1的页时，操作系统就将该位重新置为0，遇到第一个使用位为0时操作系统就选取该页淘汰。

由于在选取淘汰页的指针A移动的过程中，使用位刚被置0的页很可能因为被再次访问被置1，指针转了一圈都没看见过使用位为0的页面，然后后决定把第一页淘汰、算法退化成了FIFO。改进后引入了专门用于表示访问过了置0的指针B，它移动的比较快，两个指针一个循环，故名Clock算法。

**二十、什么是页面的抖动/颠簸？如何防止？**

​	答：分配的页框太少了，刚换出的页面马上就要换入，不停地进行页面调度，进行IO。处理器的大部分时间都将用于交换快，及请求调入页面的操作，而不是执行进程的指令，这就会大大降低系统效率。

​	为了防止抖动现象，操作系统应跟踪每个进程的驻留集，并为进程分配大于驻留集的的页框。如果所有驻留集之和增加一直超过了可用物理页框的总数，那么系统应该暂停一个进程，将其页面调出并且将其物理块分配给其他进程。

**二十一、操作系统如何管理磁盘？**

​	答：抽象的文件系统。

​	用户使用进程操作文件，每一个文件都有FCB用于存放文件自身到磁盘上的盘块号的映射，**OS封装了该映射**，因此用户只能看见文件，当用户控制进程操作文件中的字符流时，会通过FCB映射找到对应的盘块号，进程把盘块号放到**电梯队列**中，当队列中的请求被服务后，就完成了对磁盘中的数据读写操作，即完成了文件系统对磁盘的管理。

​	电梯队列值得是电梯调度算法维护的队列，遵循从磁盘的最中心到磁盘的最边缘再到磁盘的最中心的轨迹，沿途解决对盘块号的读写请求。使得磁盘寻道时间最短，并且不会出现饥饿现象。

​	**OS的文件系统**需要封装磁盘上所有文件的与盘块号的映射关系 ，形成一个目录树，整棵树存放在磁盘的一个固定格式化的区域，因为它的存在文件系统得以自举。将磁盘挂载到系统上，就是读这个区域的头部

**二十二、从源文件hello.c到可执行文件hello.exe的过程？**

预处理阶段:处理以#开头的预处理命令，生成预处理过的hello.i

编译阶段:编译生成汇编代码文件hello.s

汇编阶段:汇编成可重定位目标文件(二进制)hello.o

链接阶段:将可重定位目标文件和 printf.o 等单独预编译好的目标文件进行链接得到最终的可执行目标文件







